{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e4f1f1",
   "metadata": {},
   "source": [
    "## Update rule and game designing in infinite-horizon setting\n",
    "\n",
    "In this notebook, we benchmark the methodology for a infinite-horizon risk-sensitive inverse reinforcement learning (RL) problem by inspecting if the Gibbs measure converges to the dirac delta function. Furthermore, we design three different ways of choosing the transition probability matrix, more specifically the choice of the game $G_{n+1}$ at every round $n$. In this notebook:\n",
    "\n",
    "- Every computation of the risk-aversion measure is done with the analytical expression\n",
    "- We consider risk-aversion preferences characterized by a linear combination of the expectation and CVaR\n",
    "- The cost function can be either known or (partially) unknown\n",
    "\n",
    "**NB: This notebook will require additional coding time to put everything in a single 'learner' class instead of separate functions... We may also rename variables according to the notation we use in the preprint...**\n",
    "\n",
    "**NB: Before publicly sharing this notebook, we must adjust the notation to be consistent with the paper.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2d8aa",
   "metadata": {},
   "source": [
    "### Notation and hyperparameters\n",
    "\n",
    "We use the following notation:\n",
    "- A finite set of states $\\mathbb{X}$ with $\\text{n_states} := |\\mathbb{X}| > 0$\n",
    "- A finite set of actions $\\mathbb{A}$ with $\\text{n_actions} := |\\mathbb{A}| > 0$\n",
    "- A discount factor $r \\in [0,1)$\n",
    "- A finite set of transition probabilities, denoted by $G$, with $\\text{n_games} := |G| > 0$.\n",
    "- Finite sets of risk-aversion parameters $\\mathcal{\\kappa}$ and $\\mathcal{\\gamma}$ with $\\kappa \\in (0,1]$, $\\gamma \\in [0,1)$\n",
    "- Whether a known cost function $C_{0}: \\mathcal{X} \\rightarrow \\mathbb{R}$, or a partially unknown cost function $C_{\\ell}: \\mathcal{X} \\rightarrow \\mathbb{R}$, where we only know the values of the cost function\n",
    "- A finite set of critera $\\Xi$, denoted $\\{\\Xi_{\\ell}\\}_{\\ell=1,\\ldots,L}$ consisting of the tuples $\\{\\kappa_{\\ell}, \\gamma_{\\ell}, r_{\\ell}, C_{0}\\}_{\\ell}$ (if cost is known) or $\\{\\kappa_{\\ell}, \\gamma_{\\ell}, r_{\\ell}, C_{\\ell}\\}_{\\ell}$ (if cost is partially unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c252b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dimensions of each space\n",
    "n_states = 3\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate parameters\n",
    "assert isinstance(n_states, int) & (n_states >= 0), \"n_states must be a positive integer.\"\n",
    "assert isinstance(n_actions, int) & (n_actions >= 0), \"n_actions must be a positive integer.\"\n",
    "print('*** All parameters are validated ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7f600",
   "metadata": {},
   "source": [
    "### Definition of functions\n",
    "\n",
    "Let $\\delta_{x}$ be the Dirac measure on $x$, and suppose $\\gamma \\in [0,1)$, $\\kappa \\in (0,1]$. For simplicity, here, all risk measures we consider are of the following form:\n",
    "$$\n",
    "\\rho_{\\mu}(X) = \\int_{0}^{1} \\text{CVaR}_{1-\\alpha}(X) \\mu( \\textrm{d} \\alpha),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mu = \\gamma \\delta_{0} + (1-\\gamma) \\delta_{\\kappa}.\n",
    "$$\n",
    "This characterizes a trade-off between risk-aversion behaviors from the CVaR at level $\\kappa$, and risk-seeking behaviors from the (risk-neutral) expectation. Naturally, this notebook may be easily extended to risk measures defined as linear combinations of CVaRs at different thresholds. The function `get_analytical_risk` returns the risk measure of the cost random variable obtained for a given game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical_risk(upsilon, action, trans_probs, costs):\n",
    "    # sort costs in decreasing order\n",
    "    sorted_costs = costs[np.argsort(costs[::-1])]\n",
    "    sorted_pis = trans_probs[action,np.argsort(costs[::-1])]\n",
    "    \n",
    "    # compute CDF\n",
    "    revd_cpis = np.cumsum(sorted_pis[::-1])\n",
    "    \n",
    "    kappa_cond = int(np.sum(upsilon[0] <= revd_cpis))\n",
    "    \n",
    "    # compute risk from expectation contribution\n",
    "    risk_E = np.sum(sorted_pis*sorted_costs)\n",
    "    \n",
    "    # compute risk from CVaR contribution\n",
    "    partial_risk_CVaR = np.array([sorted_costs[i]*(revd_cpis[-(i+1)] - revd_cpis[-(i+2)]) for i in range(kappa_cond-1)])\n",
    "    risk_CVaR = (1/(1-upsilon[0]))*( \\\n",
    "                np.sum(partial_risk_CVaR) + sorted_costs[kappa_cond-1] * (revd_cpis[-(kappa_cond)] - upsilon[0]))\n",
    "    \n",
    "    # get risk of the linear combination\n",
    "    risk = upsilon[1]*risk_E + (1-upsilon[1])*risk_CVaR\n",
    "\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4ba2a",
   "metadata": {},
   "source": [
    "Define the \"value function\" as\n",
    "\\begin{equation*}\n",
    "    V^{*}_{G,\\ell}(x) = C_{\\ell}(x) + r_{\\ell} \\min_{a \\in \\mathbb{A}} \\rho_{\\mu_{\\ell}}\\Big(V^{*}_{G,\\ell}(X^{x,a}_{G})\\Big).\n",
    "\\end{equation*}\n",
    "We find the optimal $V^{*}$ using fixed-point iteration, i.e. a value iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Vstar(upsilons, games, progress=False):\n",
    "    # termination condition\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # initialize the value function\n",
    "    V_star = np.zeros((len(games), len(upsilons), n_states))\n",
    "    \n",
    "    for idx_game, game in enumerate(games):\n",
    "        if progress:\n",
    "            print(\"\\rComputation of V_star: game #{:5d}\".format(idx_game+1), end=\"\")\n",
    "        for idx_upsilon, upsilon in enumerate(upsilons):\n",
    "            \n",
    "            V = np.zeros(n_states)\n",
    "            dV = 1.0\n",
    "            n_iters = 0\n",
    "            \n",
    "            # repeat until convergence\n",
    "            while dV >= epsilon:\n",
    "                dV = 0.0\n",
    "                n_iters += 1\n",
    "                \n",
    "                for state in range(n_states):\n",
    "                    prev_V = V[state]\n",
    "\n",
    "                    # compute risk for all actions\n",
    "                    risk = np.zeros(n_actions)\n",
    "                    for action in range(n_actions):\n",
    "                        risk[action] = get_analytical_risk(upsilon, action, game[:,state,:], V)\n",
    "\n",
    "                    # update value function\n",
    "                    V[state] = upsilon[3][state] + upsilon[2]*np.min(risk)\n",
    "\n",
    "                    # get update difference\n",
    "                    dV = np.maximum(dV, np.abs(prev_V - V[state]))\n",
    "                    \n",
    "                if n_iters >= 150:\n",
    "                    break\n",
    "            \n",
    "            V_star[idx_game, idx_upsilon, :] = V\n",
    "    \n",
    "    if progress:\n",
    "        print(\" Done.\")\n",
    "    return V_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2e361",
   "metadata": {},
   "source": [
    "Define the \"regret\" as\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{\\Phi}(\\pi;G,\\ell) &= \\sum_{x \\in \\mathbb{X}} \\tilde{\\Phi}^{x}(\\pi(x);G,\\ell),\\\\\n",
    "\\tilde{\\Phi}^{x}(a;G,\\ell) &= C_\\ell(x) + r_\\ell \\rho_{\\mu_\\ell}\\Big(V^{*}_{G,\\ell}(X^{x,a}_{G})\\Big) - V^{*}_{G,\\ell}(x).\n",
    "\\end{align*}\n",
    "\n",
    "It can be viewed as the regret of the policy $\\pi$ under game $G$ and risk aversion $(C_{\\ell}, \\mu_{\\ell}, r_{\\ell})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bace20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regret(upsilons, game, pi, V_star):\n",
    "    phis = np.zeros(len(upsilons))\n",
    "    \n",
    "    for idx_upsilon, upsilon in enumerate(upsilons):\n",
    "        # compute regret for all separate states\n",
    "        phi_x = np.zeros(n_states)\n",
    "        for state in range(n_states):\n",
    "            risk = get_analytical_risk(upsilon, pi[state], game[:,state,:], V_star[idx_upsilon,:])\n",
    "            phi_x[state] = upsilon[3][state] + upsilon[2]*risk - V_star[idx_upsilon,state]\n",
    "\n",
    "        # compute regret\n",
    "        phis[idx_upsilon] = np.sum(phi_x)\n",
    "    \n",
    "    return phis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31eed2",
   "metadata": {},
   "source": [
    "Let $\\pi^{*,\\ell}$ be the optimal actions under the risk aversion $(C_{\\ell}, \\mu_{\\ell}, r_{\\ell})$. Define the \"power\" as\n",
    "\\begin{align*}\n",
    "\\tilde{\\Psi}(G;i,j) &= - \\sum_{x \\in \\mathbb{X}} \\tilde{\\Phi}^{x}(\\pi^{*,j}(x);G,i) \\tilde{\\Phi}^{x}(\\pi^{*,i}(x);G,j) \\\\\n",
    "&= - \\sum_{x \\in \\mathbb{X}} \\bigg( \\tilde{\\Phi}^{x}(1;G,i) \\tilde{\\Phi}^{x}(2;G,j) + \\tilde{\\Phi}^{x}(2;G,i) \\tilde{\\Phi}^{x}(1;G,j) \\bigg).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa782688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(upsilons, games, V_star, progress=False):\n",
    "    psis = np.zeros((len(games), len(upsilons), len(upsilons)))\n",
    "    \n",
    "    # loop for each lambda\n",
    "    for idx_game, game in enumerate(games):\n",
    "        if progress:\n",
    "            print(\"\\rPower computation: game #{:5d}\".format(idx_game+1), end=\"\")            \n",
    "        \n",
    "        # loop for each criterion\n",
    "        for idx_upsilon1, upsilon1 in enumerate(upsilons):\n",
    "            # loop for other criteria\n",
    "            for idx_upsilon2, upsilon2 in enumerate(upsilons):\n",
    "                # make sure \\psi(\\lambda, \\mu_i, \\mu_i) is -\\inf\n",
    "                if idx_upsilon1 == idx_upsilon2:\n",
    "                    psis[idx_game, idx_upsilon1, idx_upsilon1] = -np.inf\n",
    "                else:\n",
    "                    # get regret for all pairs of actions / upsilons\n",
    "                    phi_x_a0_u1 = np.zeros(n_states)\n",
    "                    phi_x_a1_u1 = np.zeros(n_states)\n",
    "                    phi_x_a0_u2 = np.zeros(n_states)\n",
    "                    phi_x_a1_u2 = np.zeros(n_states)\n",
    "\n",
    "                    for state in range(n_states):\n",
    "                        phi_x_a0_u1[state] = upsilon1[3][state] - V_star[idx_game,idx_upsilon1,state] \\\n",
    "                            + upsilon1[2]*get_analytical_risk(upsilon1, 0, game[:,state,:], V_star[idx_game,idx_upsilon1,:])\n",
    "                        phi_x_a1_u1[state] = upsilon1[3][state] - V_star[idx_game,idx_upsilon1,state] \\\n",
    "                            + upsilon1[2]*get_analytical_risk(upsilon1, 1, game[:,state,:], V_star[idx_game,idx_upsilon1,:])\n",
    "                        phi_x_a0_u2[state] = upsilon2[3][state] - V_star[idx_game,idx_upsilon2,state] \\\n",
    "                            + upsilon2[2]*get_analytical_risk(upsilon2, 0, game[:,state,:], V_star[idx_game,idx_upsilon2,:])\n",
    "                        phi_x_a1_u2[state] = upsilon2[3][state] - V_star[idx_game,idx_upsilon2,state] \\\n",
    "                            + upsilon2[2]*get_analytical_risk(upsilon2, 1, game[:,state,:], V_star[idx_game,idx_upsilon2,:])\n",
    "\n",
    "                    # compute the power\n",
    "                    psis[idx_game, idx_upsilon1, idx_upsilon2] = -1 * np.sum(phi_x_a0_u1*phi_x_a1_u2 \\\n",
    "                                                                             + phi_x_a1_u1*phi_x_a0_u2)\n",
    "    \n",
    "    if progress:\n",
    "        print(\" Done.\")\n",
    "    return psis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934564c",
   "metadata": {},
   "source": [
    "The `select_nextgame` function determines which transition probability matrix to use according to three different methods:\n",
    "\n",
    "- **\"random\"** randomly selects a game\n",
    "- **\"largest\"** selects the next game according to\n",
    "\\begin{equation*}\n",
    "\\arg\\!\\min_{G} \\tilde{\\Psi}(G;\\eta,\\zeta),\n",
    "\\end{equation*}\n",
    "where $(\\eta,\\zeta)$ is the pair of entries with the largest and second largest probabilities assigned by $\\mathbb{Q}$.\n",
    "- **\"expected\"** selects the next game according to\n",
    "\\begin{equation*}\n",
    "\\arg\\!\\min_{G} \\mathbb{E}_{(\\eta,\\zeta) \\sim \\mathbb{Q}|_{\\eta \\neq \\zeta}} \\Big[ \\tilde{\\Psi}(G;\\eta,\\zeta) \\Big].\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nextgame(psis, upsilons, gibbs_meas, games, n_sims, method):\n",
    "    if method==\"random\":\n",
    "        next_game = np.random.randint(len(games))\n",
    "    \n",
    "    elif method==\"largest\":\n",
    "        # obtain the two risk-aversions with the largest Gibbs value\n",
    "        largest = np.partition(gibbs_meas, -2)[-2:]\n",
    "        if largest[0] == largest[1]:\n",
    "            upsilon_largest = np.random.choice(np.where(gibbs_meas == largest[1])[0], size=2)\n",
    "        else:\n",
    "            upsilon_largest = np.array([np.where(gibbs_meas == largest[1])[0][0],\n",
    "                                        np.random.choice(np.where(gibbs_meas == largest[0])[0])])\n",
    "        \n",
    "        next_game = np.argmin( psis[ :, upsilon_largest[0], upsilon_largest[1]] )\n",
    "       \n",
    "    elif method==\"expected\":\n",
    "        # sample two criteria (without replacement) according to the Gibbs measure\n",
    "        upsilon_rand = np.stack([np.random.choice(len(upsilons),size=2,replace=False,p=gibbs_meas) for i in range(n_sims)])\n",
    "        \n",
    "        # compute the expectation of the difference in power\n",
    "        expected_powers = np.zeros(len(games))\n",
    "        for idx_game, game in enumerate(games):\n",
    "            expected_powers[idx_game] = np.mean( psis[ idx_game, upsilon_rand[:,0], upsilon_rand[:,1]] )\n",
    "\n",
    "        next_game = np.argmin(expected_powers)\n",
    "        \n",
    "    else:\n",
    "        ValueError(\"Selection method is unknown ('random' or 'largest' or 'expected').\")\n",
    "        \n",
    "    return next_game, games[next_game, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1930fc",
   "metadata": {},
   "source": [
    "The `learn` function runs the main learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(games, # set of games\n",
    "          V_star, # V^{*} for all games\n",
    "          psis, # power for all games\n",
    "          upsilons, # set of criteria for the learner\n",
    "          upsilon0, # criterion for the expert\n",
    "          n_rounds, # number of rounds\n",
    "          select_method, # string for the selection method when designing games at each episode\n",
    "          lr, # learning rate for the Gibbs measure\n",
    "          print_progress=None): # number of episodes before printing the progress\n",
    "\n",
    "    # initialize Gibbs measure\n",
    "    phis_hat = np.zeros(len(upsilons))\n",
    "    gibbs_meas = np.exp(phis_hat-np.max(phis_hat)) / np.sum(np.exp(phis_hat-np.max(phis_hat)))\n",
    "    \n",
    "    true_gibbs = np.zeros((n_rounds, len(upsilons)))\n",
    "        \n",
    "    # algorithm\n",
    "    for k in range(n_rounds):\n",
    "\n",
    "        # select the lambda with highest power\n",
    "        idx_game, game = select_nextgame(psis, upsilons, gibbs_meas, games, n_sims=100, method=select_method)\n",
    "\n",
    "        # get optimal action from analytical risk\n",
    "        opt_pi = np.zeros(n_states, dtype=int)\n",
    "        opt_V_star = compute_Vstar([upsilon0], game[np.newaxis, :], progress=False).squeeze()\n",
    "        for state in range(n_states):\n",
    "            all_actions = np.zeros(n_actions)\n",
    "            for action in range(n_actions):\n",
    "                all_actions[action] = get_analytical_risk(upsilon0, action, game[:,state,:], opt_V_star)\n",
    "            opt_pi[state] = np.argmin(all_actions)\n",
    "        \n",
    "        # compute the potential/energy for all upsilons\n",
    "        phis_k = compute_regret(upsilons, game, opt_pi, V_star[idx_game,:,:])\n",
    "        ###print(np.array_str(phis_k, precision=5, suppress_small=True))\n",
    "        \n",
    "        # normalize the potentials\n",
    "        phis_normalized = phis_k - np.mean(phis_k)\n",
    "\n",
    "        # update sum of potentials\n",
    "        phis_hat -= phis_normalized\n",
    "\n",
    "        # update Gibbs measure (subtracting the maximum for numerical stability)\n",
    "        gibbs_meas = np.exp(lr*(phis_hat-np.max(phis_hat))) / np.sum(np.exp(lr*(phis_hat-np.max(phis_hat))))\n",
    "        \n",
    "        # keep track Gibbs measure for true risk-aversion\n",
    "        true_gibbs[k,...] = gibbs_meas\n",
    "        \n",
    "        # print progress\n",
    "        if (print_progress is not None) and (k % print_progress == 0 or k == n_rounds - 1):\n",
    "            print(\"Episode: {:3d};\".format(k+1),\n",
    "                  \"Best criterion: {:2d}\".format(np.argmax(gibbs_meas) + 1),\n",
    "                  \"with kappa: {:.2f}\".format(upsilons[np.argmax(gibbs_meas)][0]),\n",
    "                  \"  gamma: {:.2f}\".format(upsilons[np.argmax(gibbs_meas)][1]),\n",
    "                  \"  discount: {:.2f}\".format(upsilons[np.argmax(gibbs_meas)][2]),\n",
    "                  \"  cost function: \", np.array_str(upsilons[np.argmax(gibbs_meas)][3], precision=2))        \n",
    "            print(\"Gibbs:\", np.array_str(gibbs_meas, precision=2, suppress_small=True))\n",
    "    \n",
    "    return true_gibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196508e",
   "metadata": {},
   "source": [
    "### Initialization of Hyperparameters\n",
    "\n",
    "Once we perform the training of the $V$, we run the learning algorithm. The expert specifies its criterion, denoted $\\Xi_0$, while the learner specifies the set of criteria, the number of games to explore when designing the next game, and the learning rate. Note here that the expert criterion may be a different criterion than the ones in the criterion space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXPERT AND LEARNER PARAMETERS\n",
    "\"\"\"\n",
    "### set of experiments 1\n",
    "upsilon0 = (0.4, 0.2, 0.4, np.array([1.0, 0.5, 0.0]))\n",
    "kappas = np.array([0.2, 0.3, 0.4])\n",
    "gammas = np.array([0.2, 0.5])\n",
    "discounts = np.array([0.2, 0.4, 0.6])\n",
    "unknown_cost = True\n",
    "costs = np.array([[1.0, 0.5, 0.0],\n",
    "                [0.5, 1.0, 0.0]])\n",
    "\n",
    "# ### set of experiments 2\n",
    "# upsilon0 = (0.25, 0.0, 0.8, np.array([1.0, 0.5, 0.0]))\n",
    "# kappas = np.array([0.25])\n",
    "# gammas = np.array([0.0])\n",
    "# discounts = np.linspace(0.1, 0.9, 21)\n",
    "# unknown_cost = False\n",
    "# costs = np.array([[1.0, 0.5, 0.0]])\n",
    "\n",
    "upsilons = list(itertools.product(kappas, gammas, discounts, costs))\n",
    "\n",
    "\"\"\"\n",
    "ALGORITHM PARAMETERS\n",
    "\"\"\"\n",
    "n_games = 500 # number of games\n",
    "lr = 4 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate all parameters\n",
    "assert isinstance(n_games, int) & (n_games >= 0), \"n_games must be a positive integer.\"\n",
    "assert np.all((kappas > 0) & (kappas < 1)), \"All kappas must be in (0,1).\"\n",
    "assert np.all((gammas >= 0) & (gammas < 1)), \"All gammas must be in [0,1).\"\n",
    "assert np.all((discounts >= 0) & (discounts < 1)), \"All discount factors must be in [0,1).\"\n",
    "assert len(upsilon0) == 4, \"upsilon0 must contain a (kappa, gamma, discount factor, cost function).\"\n",
    "print('*** All parameters are validated ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b469328",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Agent criterion --  kappa: {:.2f}\".format(upsilon0[0]),\n",
    "      \"  gamma: {:.2f}\".format(upsilon0[1]),\n",
    "      \"  discount: {:.2f}\".format(upsilon0[2]),\n",
    "      \"  cost function: \", np.array_str(upsilon0[3], precision=2))\n",
    "print(\"--------------\")\n",
    "print('Criterion space (', 'unknown' if unknown_cost else 'known', ' cost):', sep='')\n",
    "for idx, upsilon in enumerate(upsilons):\n",
    "    print(\"Criterion #{:2d}\".format(idx+1),\n",
    "          \" --  kappa: {:.2f}  \".format(upsilon[0]), \n",
    "          \"  gamma: {:.2f}  \".format(upsilon[1]),\n",
    "          \"  discount: {:.2f}  \".format(upsilon[2]),\n",
    "          \"  cost function: \", np.array_str(upsilon[3], precision=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6efb4",
   "metadata": {},
   "source": [
    "### Computation of the power for all games\n",
    "\n",
    "We first sample many games, and then compute the power of all games with respect to all pairs of risk-aversions. This is independent of the selection method for the next game, which is why we execute this outside the `learn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6f58e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# generate a set of games (random transition probabilities)\n",
    "z = np.random.randn(n_games, n_actions, n_states, n_states)\n",
    "games = np.exp(z) / np.sum(np.exp(z), axis=3, keepdims=True)\n",
    "\n",
    "# compute V_star for all games\n",
    "V_star = compute_Vstar(upsilons, games, progress=True)\n",
    "\n",
    "# compute power for all games\n",
    "psis = compute_power(upsilons, games, V_star, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa733dd",
   "metadata": {},
   "source": [
    "### Learning algorithm with uniform game design\n",
    "\n",
    "In this setting, we first validate that the Gibbs measure converges to the dirac delta function when using uniformly randomized games at each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a81f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "gibbs = \\\n",
    "    learn(games=games,\n",
    "          V_star=V_star,\n",
    "          psis=psis,\n",
    "          upsilons=upsilons,\n",
    "          upsilon0=upsilon0,\n",
    "          n_rounds=10_000,\n",
    "          select_method=\"random\",\n",
    "          lr=lr,\n",
    "          print_progress=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# lines\n",
    "a = gibbs[len(gibbs)//2,:]\n",
    "legend = [\"_nolegend_\" if a[i] < np.partition(a, -4)[-4] \\\n",
    "                   else r\"$\\Upsilon_{{{:2d}}}$\".format(i+1)+\\\n",
    "                          r\": $\\kappa=$ {:.3f}\".format(upsilons[i][0])+ \\\n",
    "                          r\"  $\\gamma=$ {:.3f}\".format(upsilons[i][1])+ \\\n",
    "                          r\"  $r=:$ {:.2f}  \".format(upsilon[2]) + \\\n",
    "                          \"  cost: \"+ np.array_str(upsilons[i][3], precision=2) for i in range(len(a))]\n",
    "\n",
    "axes.plot(np.arange(len(gibbs)),\n",
    "          gibbs,\n",
    "          linewidth=2.0,\n",
    "          label=legend)\n",
    "\n",
    "# aesthetics\n",
    "# axes.set_title('Convergence of the Gibbs measure', fontsize=24)\n",
    "axes.set_xlabel('Rounds', fontsize=20)\n",
    "axes.set_ylabel(r\"Values of $\\mathbb{Q}_{N}$\", fontsize=20)\n",
    "axes.set_xlim(0, len(gibbs))\n",
    "axes.set_ylim(0, 1)\n",
    "axes.legend(loc=9, bbox_to_anchor=(0.5,-0.12), fontsize=18)\n",
    "axes.tick_params(axis='both', labelsize=14)\n",
    "plt.savefig(\"figures/uniform-gibbs-infinite.pdf\", transparent=False, bbox_inches='tight')\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906806d",
   "metadata": {},
   "source": [
    "### Learning algorithm with minimization of power for largest risk-aversions\n",
    "\n",
    "In this setting, we verify if the Gibbs measure converges faster when designing the next game according to the power of the largest risk-aversion values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ebb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "gibbs = \\\n",
    "    learn(games=games,\n",
    "          V_star=V_star,\n",
    "          psis=psis,\n",
    "          upsilons=upsilons,\n",
    "          upsilon0=upsilon0,\n",
    "          n_rounds=500,\n",
    "          select_method=\"largest\",\n",
    "          lr=lr,\n",
    "          print_progress=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17882a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# lines\n",
    "a = gibbs[len(gibbs)//2,:]\n",
    "legend = [\"_nolegend_\" if a[i] < np.partition(a, -4)[-4] \\\n",
    "                   else r\"$\\Upsilon_{{{:2d}}}$\".format(i+1)+\\\n",
    "                          r\": $\\kappa=$ {:.3f}\".format(upsilons[i][0])+ \\\n",
    "                          r\"  $\\gamma=$ {:.3f}\".format(upsilons[i][1])+ \\\n",
    "                          r\"  $r=:$ {:.2f}  \".format(upsilon[2]) + \\\n",
    "                          \"  cost: \"+ np.array_str(upsilons[i][3], precision=2) for i in range(len(a))]\n",
    "\n",
    "axes.plot(np.arange(len(gibbs)),\n",
    "          gibbs,\n",
    "          linewidth=2.0,\n",
    "          label=legend)\n",
    "\n",
    "# aesthetics\n",
    "# axes.set_title('Convergence of the Gibbs measure', fontsize=24)\n",
    "axes.set_xlabel('Rounds', fontsize=20)\n",
    "axes.set_ylabel(r\"Values of $\\mathbb{Q}_{N}$\", fontsize=20)\n",
    "axes.set_xlim(0, 500)\n",
    "axes.set_ylim(0, 1)\n",
    "axes.legend(loc=9, bbox_to_anchor=(0.5,-0.12), fontsize=18)\n",
    "axes.tick_params(axis='both', labelsize=14)\n",
    "plt.savefig(\"figures/largest-gibbs-infinite.pdf\", transparent=False, bbox_inches='tight')\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc004f2",
   "metadata": {},
   "source": [
    "### Learning algorithm with minimization of expected power\n",
    "\n",
    "In this setting, we verify if the Gibbs measure converges faster when designing the next game according to the expected power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489497c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "gibbs = \\\n",
    "    learn(games=games,\n",
    "          V_star=V_star,\n",
    "          psis=psis,\n",
    "          upsilons=upsilons,\n",
    "          upsilon0=upsilon0,\n",
    "          n_rounds=500,\n",
    "          select_method=\"expected\",\n",
    "          lr=lr,\n",
    "          print_progress=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# lines\n",
    "a = gibbs[len(gibbs)//2,:]\n",
    "legend = [\"_nolegend_\" if a[i] < np.partition(a, -4)[-4] \\\n",
    "                   else r\"$\\Upsilon_{{{:2d}}}$\".format(i+1)+\\\n",
    "                          r\": $\\kappa=$ {:.3f}\".format(upsilons[i][0])+ \\\n",
    "                          r\"  $\\gamma=$ {:.3f}\".format(upsilons[i][1])+ \\\n",
    "                          r\"  $r=:$ {:.2f}  \".format(upsilon[2]) + \\\n",
    "                          \"  cost: \"+ np.array_str(upsilons[i][3], precision=2) for i in range(len(a))]\n",
    "\n",
    "axes.plot(np.arange(len(gibbs)),\n",
    "          gibbs,\n",
    "          linewidth=2.0,\n",
    "          label=legend)\n",
    "\n",
    "# aesthetics\n",
    "# axes.set_title('Convergence of the Gibbs measure', fontsize=24)\n",
    "axes.set_xlabel('Rounds', fontsize=20)\n",
    "axes.set_ylabel(r\"Values of $\\mathbb{Q}_{N}$\", fontsize=20)\n",
    "axes.set_xlim(0, 500)\n",
    "axes.set_ylim(0, 1)\n",
    "axes.legend(loc=9, bbox_to_anchor=(0.5,-0.12), fontsize=18)\n",
    "axes.tick_params(axis='both', labelsize=14)\n",
    "plt.savefig(\"figures/expected-gibbs-infinite.pdf\", transparent=False, bbox_inches='tight')\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af7764",
   "metadata": {},
   "source": [
    "### Convergence speed visualizations\n",
    "\n",
    "In this setting, we perform multiple runs of the learning algorithm for the different game-design rules. This allows an estimation of the distribution for $\\mathbb{Q}_{N}(\\{0\\})$, i.e. the Gibbs measure value for the expert's true risk-aversion, at each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6910fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4321)\n",
    "n_runs = 25\n",
    "n_rounds1 = 5000\n",
    "n_rounds2 = 5000\n",
    "n_games = 500\n",
    "lr = 4\n",
    "\n",
    "idx_upsilon0 = [idx for idx,ii in enumerate(upsilons) if (ii[0] == upsilon0[0]) and \\\n",
    "                                                            (ii[1] == upsilon0[1]) and \\\n",
    "                                                            (ii[2] == upsilon0[2]) and \\\n",
    "                                                            (ii[3] == upsilon0[3]).all()][0]\n",
    "\n",
    "unif_gibbs = np.zeros((n_runs,n_rounds1,len(upsilons)))\n",
    "largest_gibbs = np.zeros((n_runs,n_rounds2,len(upsilons)))\n",
    "expected_gibbs = np.zeros((n_runs,n_rounds2,len(upsilons)))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    \n",
    "    print(\"\\rProgress: run #{:5d} of {:5d}\".format(run+1, n_runs), end=\"\")\n",
    "    \n",
    "    # generate a set of games (random transition probabilities)\n",
    "    z = np.random.randn(n_games, n_actions, n_states, n_states)\n",
    "    games = np.exp(z) / np.sum(np.exp(z), axis=3, keepdims=True)\n",
    "\n",
    "    # compute V_star for all games\n",
    "    V_star = compute_Vstar(upsilons, games, progress=False)\n",
    "\n",
    "    # compute power for all games\n",
    "    psis = compute_power(upsilons, games, V_star, progress=False)\n",
    "\n",
    "    unif_gibbs[run,:] = \\\n",
    "        learn(games=games,\n",
    "              V_star=V_star,\n",
    "              psis=psis,\n",
    "              upsilons=upsilons,\n",
    "              upsilon0=upsilon0,\n",
    "              n_rounds=n_rounds1,\n",
    "              select_method=\"random\",\n",
    "              lr=lr)\n",
    "    largest_gibbs[run,:] = \\\n",
    "        learn(games=games,\n",
    "              V_star=V_star,\n",
    "              psis=psis,\n",
    "              upsilons=upsilons,\n",
    "              upsilon0=upsilon0,\n",
    "              n_rounds=n_rounds2,\n",
    "              select_method=\"largest\",\n",
    "              lr=lr)\n",
    "    expected_gibbs[run,:] = \\\n",
    "        learn(games=games,\n",
    "              V_star=V_star,\n",
    "              psis=psis,\n",
    "              upsilons=upsilons,\n",
    "              upsilon0=upsilon0,\n",
    "              n_rounds=n_rounds2,\n",
    "              select_method=\"expected\",\n",
    "              lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45335859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,6))\n",
    "\n",
    "largest_upsilon0 = largest_gibbs[...,idx_upsilon0]\n",
    "expected_upsilon0 = expected_gibbs[...,idx_upsilon0]\n",
    "unif_upsilon0 = unif_gibbs[...,idx_upsilon0]\n",
    "\n",
    "# lines\n",
    "axes.plot(np.arange(n_rounds2),\n",
    "          np.mean(largest_upsilon0, axis=0),\n",
    "          label='largest', color='C0', linewidth=1.5)\n",
    "axes.plot(np.arange(n_rounds2),\n",
    "          np.mean(expected_upsilon0, axis=0),\n",
    "          label='expected', color='C1', linewidth=1.5)\n",
    "axes.plot(np.arange(n_rounds1),\n",
    "          np.mean(unif_upsilon0, axis=0),\n",
    "          label='uniform', color='C2', linewidth=1.5)\n",
    "\n",
    "# shades\n",
    "axes.fill_between(np.arange(n_rounds2),\n",
    "                  np.quantile(largest_upsilon0, 0.9, axis=0),\n",
    "                  np.quantile(largest_upsilon0, 0.1, axis=0),\n",
    "                  facecolor='C0', alpha=0.3)\n",
    "axes.fill_between(np.arange(n_rounds2),\n",
    "                  np.quantile(expected_upsilon0, 0.9, axis=0),\n",
    "                  np.quantile(expected_upsilon0, 0.1, axis=0),\n",
    "                  facecolor='C1', alpha=0.3)\n",
    "axes.fill_between(np.arange(n_rounds1),\n",
    "                  np.quantile(unif_upsilon0, 0.9, axis=0),\n",
    "                  np.quantile(unif_upsilon0, 0.1, axis=0),\n",
    "                  facecolor='C2', alpha=0.3)\n",
    "\n",
    "# aesthetics\n",
    "# axes.set_title('Convergence of the Gibbs measure to the true risk-aversion', fontsize = 24)\n",
    "axes.set_xlabel(r'Rounds (log-scale)', fontsize=20)\n",
    "axes.set_ylabel(r\"Value of $\\tilde{\\mathbb{Q}}_{N}(\\{0\\})$\", fontsize=20)\n",
    "axes.legend(loc='upper left', fontsize=18)\n",
    "axes.set_xlim(1, n_rounds1)\n",
    "axes.set_xscale('log')\n",
    "axes.xaxis.set_major_formatter(ScalarFormatter())\n",
    "axes.set_xticks([1,10,100,1000,5000])\n",
    "axes.set_ylim(0, 1)\n",
    "axes.tick_params(axis='both', labelsize=14)\n",
    "plt.savefig(\"figures/gibbs-infinite.pdf\", transparent=False, bbox_inches='tight')\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4eff43",
   "metadata": {},
   "source": [
    "### Learning rate visualizations\n",
    "\n",
    "In this setting, we perform multiple runs of the learning algorithm for the different game-design rules, but vary the learning rate to understand the behavior of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4321)\n",
    "n_runs = 25\n",
    "n_rounds = 50\n",
    "n_games = 500\n",
    "lrs = np.linspace(0.1, 10.0, 25)\n",
    "\n",
    "idx_upsilon0 = [idx for idx,ii in enumerate(upsilons) if (ii[0] == upsilon0[0]) and \\\n",
    "                                                            (ii[1] == upsilon0[1]) and \\\n",
    "                                                            (ii[2] == upsilon0[2]) and \\\n",
    "                                                            (ii[3] == upsilon0[3]).all()][0]\n",
    "\n",
    "unif_gibbs = np.zeros((len(lrs),n_runs,n_rounds,len(upsilons)))\n",
    "largest_gibbs = np.zeros((len(lrs),n_runs,n_rounds,len(upsilons)))\n",
    "expected_gibbs = np.zeros((len(lrs),n_runs,n_rounds,len(upsilons)))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    \n",
    "    print(\"\\rProgress: run #{:5d} of {:5d}\".format(run+1, n_runs), end=\"\")\n",
    "    \n",
    "    # generate a set of games (random transition probabilities)\n",
    "    z = np.random.randn(n_games, n_actions, n_states, n_states)\n",
    "    games = np.exp(z) / np.sum(np.exp(z), axis=3, keepdims=True)\n",
    "\n",
    "    # compute V_star for all games\n",
    "    V_star = compute_Vstar(upsilons, games, progress=False)\n",
    "\n",
    "    # compute power for all games\n",
    "    psis = compute_power(upsilons, games, V_star, progress=False)\n",
    "    \n",
    "    for idx_lr, lr in enumerate(lrs):\n",
    "\n",
    "        unif_gibbs[idx_lr,run,...] = \\\n",
    "            learn(games=games,\n",
    "                  V_star=V_star,\n",
    "                  psis=psis,\n",
    "                  upsilons=upsilons,\n",
    "                  upsilon0=upsilon0,\n",
    "                  n_rounds=n_rounds,\n",
    "                  select_method=\"random\",\n",
    "                  lr=lr)\n",
    "        largest_gibbs[idx_lr,run,...] = \\\n",
    "            learn(games=games,\n",
    "                  V_star=V_star,\n",
    "                  psis=psis,\n",
    "                  upsilons=upsilons,\n",
    "                  upsilon0=upsilon0,\n",
    "                  n_rounds=n_rounds,\n",
    "                  select_method=\"largest\",\n",
    "                  lr=lr)\n",
    "        expected_gibbs[idx_lr,run,...] = \\\n",
    "            learn(games=games,\n",
    "                  V_star=V_star,\n",
    "                  psis=psis,\n",
    "                  upsilons=upsilons,\n",
    "                  upsilon0=upsilon0,\n",
    "                  n_rounds=n_rounds,\n",
    "                  select_method=\"expected\",\n",
    "                  lr=lr)\n",
    "\n",
    "print(\" Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ea689",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,6))\n",
    "\n",
    "largest_upsilon0 = largest_gibbs[...,-1,idx_upsilon0]\n",
    "expected_upsilon0 = expected_gibbs[...,-1,idx_upsilon0]\n",
    "unif_upsilon0 = unif_gibbs[...,-1,idx_upsilon0]\n",
    "\n",
    "# lines\n",
    "axes.plot(lrs,\n",
    "          np.mean(largest_upsilon0, axis=1),\n",
    "          label='largest', color='C0', linewidth=1.5)\n",
    "axes.plot(lrs,\n",
    "          np.mean(expected_upsilon0, axis=1),\n",
    "          label='expected', color='C1', linewidth=1.5)\n",
    "axes.plot(lrs,\n",
    "          np.mean(unif_upsilon0, axis=1),\n",
    "          label='uniform', color='C2', linewidth=1.5)\n",
    "\n",
    "# shades\n",
    "axes.fill_between(lrs,\n",
    "                  np.quantile(largest_upsilon0, 0.9, axis=1),\n",
    "                  np.quantile(largest_upsilon0, 0.1, axis=1),\n",
    "                  facecolor='C0', alpha=0.3)\n",
    "axes.fill_between(lrs,\n",
    "                  np.quantile(expected_upsilon0, 0.9, axis=1),\n",
    "                  np.quantile(expected_upsilon0, 0.1, axis=1),\n",
    "                  facecolor='C1', alpha=0.3)\n",
    "axes.fill_between(lrs,\n",
    "                  np.quantile(unif_upsilon0, 0.9, axis=1),\n",
    "                  np.quantile(unif_upsilon0, 0.1, axis=1),\n",
    "                  facecolor='C2', alpha=0.3)\n",
    "\n",
    "# aesthetics\n",
    "# axes.set_title('Convergence of the Gibbs measure to the true risk-aversion\\n after a given number of rounds', fontsize = 24)\n",
    "axes.set_xlabel('Learning rate', fontsize=20)\n",
    "axes.set_ylabel(r\"Value of $\\tilde{\\mathbb{Q}}_{N}(\\{0\\})$\", fontsize=20)\n",
    "axes.legend(loc='upper left', fontsize=18)\n",
    "axes.set_xlim(np.min(lrs), np.max(lrs))\n",
    "axes.set_ylim(0, 1)\n",
    "axes.tick_params(axis='both', labelsize=14)\n",
    "plt.savefig(\"figures/all-lrs-infinite.pdf\", transparent=False, bbox_inches='tight')\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
